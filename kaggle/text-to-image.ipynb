{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3ee460",
   "metadata": {},
   "source": [
    "# Artistic QR Code Generation on Kaggle\n",
    "\n",
    "This notebook generates artistic images with embedded scannable QR codes using Stable Diffusion and ControlNet.\n",
    "\n",
    "## Features\n",
    "- Generate artistic images with embedded QR codes\n",
    "- Support for both ControlNet and post-processing embedding methods\n",
    "- Automatic QR code validation\n",
    "- GPU acceleration support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "783f5602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:02:50.424127Z",
     "iopub.status.busy": "2026-02-25T07:02:50.423813Z",
     "iopub.status.idle": "2026-02-25T07:03:04.662171Z",
     "shell.execute_reply": "2026-02-25T07:03:04.661067Z",
     "shell.execute_reply.started": "2026-02-25T07:02:50.424101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Core ML libraries\n",
    "!pip install -q diffusers>=0.21.0 transformers>=4.30.0 accelerate>=0.20.0\n",
    "\n",
    "# Image processing\n",
    "!pip install -q qrcode[pil]>=7.4.2 Pillow>=10.0.0\n",
    "\n",
    "# QR code validation (OpenCV is more reliable on Kaggle than pyzbar)\n",
    "!pip install -q opencv-python-headless>=4.8.0\n",
    "\n",
    "# Optional: Scientific computing (for edge enhancement)\n",
    "!pip install -q scipy numpy\n",
    "\n",
    "# Note: pyzbar requires system libraries (libzbar) that are not available on Kaggle\n",
    "# QR validation uses OpenCV instead, which works reliably in Kaggle environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ab3c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:04.664576Z",
     "iopub.status.busy": "2026-02-25T07:03:04.664259Z",
     "iopub.status.idle": "2026-02-25T07:03:04.699514Z",
     "shell.execute_reply": "2026-02-25T07:03:04.698995Z",
     "shell.execute_reply.started": "2026-02-25T07:03:04.664549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "⭐ Using Stable Diffusion XL (SDXL) for best artistic quality + QR awareness\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.64 GB\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import qrcode\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# SDXL imports for better artistic quality (with fallback to SD v1.5)\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline, \n",
    "    StableDiffusionXLControlNetPipeline,\n",
    "    StableDiffusionPipeline,  # Fallback\n",
    "    StableDiffusionControlNetPipeline,  # Fallback\n",
    "    ControlNetModel,\n",
    "    AutoencoderKL\n",
    ")\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"⭐ Using Stable Diffusion XL (SDXL) for best artistic quality + QR awareness\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "098b59be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:04.700616Z",
     "iopub.status.busy": "2026-02-25T07:03:04.700324Z",
     "iopub.status.idle": "2026-02-25T07:03:04.707720Z",
     "shell.execute_reply": "2026-02-25T07:03:04.707040Z",
     "shell.execute_reply.started": "2026-02-25T07:03:04.700597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /kaggle/working/outputs\n",
      "HF Cache: /kaggle/working/hf_cache\n"
     ]
    }
   ],
   "source": [
    "# Kaggle-specific paths\n",
    "KAGGLE_WORKING = \"/kaggle/working\"\n",
    "KAGGLE_INPUT = \"/kaggle/input\"\n",
    "OUTPUT_DIR = os.path.join(KAGGLE_WORKING, \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Set up Hugging Face cache (use working directory)\n",
    "HF_CACHE = os.path.join(KAGGLE_WORKING, \"hf_cache\")\n",
    "os.makedirs(HF_CACHE, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.join(HF_CACHE, \"hub\")\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"HF Cache: {HF_CACHE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b1619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:08.271450Z",
     "iopub.status.busy": "2026-02-25T07:03:08.270745Z",
     "iopub.status.idle": "2026-02-25T07:03:08.284552Z",
     "shell.execute_reply": "2026-02-25T07:03:08.283898Z",
     "shell.execute_reply.started": "2026-02-25T07:03:08.271421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Artistic QR Pipeline Class (Kaggle-adapted)\n",
    "class ArtisticQRPipeline:\n",
    "    \"\"\"Complete pipeline for generating artistic images with embedded QR codes.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_id: str = \"runwayml/stable-diffusion-v1-5\",\n",
    "                 cache_dir: str = None,\n",
    "                 device: Optional[str] = None):\n",
    "        self.model_id = model_id\n",
    "        self.cache_dir = cache_dir or HF_CACHE\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pipe = None\n",
    "        self.controlnet_pipe = None\n",
    "        self.controlnet_model = None\n",
    "        \n",
    "    def create_qr_code(self, data: str, size: int = 512, border: int = 4, \n",
    "                       error_correction: str = \"H\") -> Image.Image:\n",
    "        \"\"\"\n",
    "        Create a QR code with specified parameters.\n",
    "        Optimized for maximum scannability (like qrcode-ai.com approach).\n",
    "        \"\"\"\n",
    "        error_levels = {\n",
    "            'L': qrcode.constants.ERROR_CORRECT_L,\n",
    "            'M': qrcode.constants.ERROR_CORRECT_M,\n",
    "            'Q': qrcode.constants.ERROR_CORRECT_Q,\n",
    "            'H': qrcode.constants.ERROR_CORRECT_H\n",
    "        }\n",
    "        \n",
    "        # Auto-select version for optimal QR code size (like qrcode-ai.com)\n",
    "        qr = qrcode.QRCode(\n",
    "            version=None,  # Auto-select version based on data\n",
    "            error_correction=error_levels.get(error_correction.upper(), qrcode.constants.ERROR_CORRECT_H),\n",
    "            box_size=10,\n",
    "            border=border,\n",
    "        )\n",
    "        qr.add_data(data)\n",
    "        qr.make(fit=True)\n",
    "        \n",
    "        # Create high-quality QR code with pure black/white for maximum contrast\n",
    "        qr_img = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
    "        \n",
    "        # High-quality resampling for better scannability\n",
    "        qr_img = qr_img.resize((size, size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Ensure pure black/white (no grayscale) for maximum scannability\n",
    "        qr_array = np.array(qr_img)\n",
    "        if len(qr_array.shape) == 2:  # Grayscale\n",
    "            qr_array = (qr_array > 127).astype(np.uint8) * 255\n",
    "            qr_img = Image.fromarray(qr_array, mode='L').convert('RGB')\n",
    "        else:  # RGB\n",
    "            # Convert to binary black/white\n",
    "            gray = np.dot(qr_array[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "            binary = (gray > 127).astype(np.uint8) * 255\n",
    "            qr_img = Image.fromarray(np.stack([binary, binary, binary], axis=2), mode='RGB')\n",
    "        \n",
    "        return qr_img\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the Stable Diffusion model.\"\"\"\n",
    "        if self.pipe is not None:\n",
    "            return self.pipe\n",
    "        \n",
    "        print(f\"Loading Stable Diffusion model: {self.model_id}\")\n",
    "        dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        \n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=dtype,\n",
    "            cache_dir=self.cache_dir\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            try:\n",
    "                self.pipe.enable_xformers_memory_efficient_attention()\n",
    "            except:\n",
    "                print(\"xformers not available, continuing without it\")\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        return self.pipe\n",
    "    \n",
    "    def load_controlnet_model(self):\n",
    "        \"\"\"\n",
    "        Load ControlNet model for QR code generation.\n",
    "        Note: SD v1.5 ControlNet models are NOT compatible with SDXL.\n",
    "        We use SD v1.5 for ControlNet (proven compatibility) even if base model is SDXL.\n",
    "        \"\"\"\n",
    "        if self.controlnet_pipe is not None:\n",
    "            return self.controlnet_pipe\n",
    "        \n",
    "        print(\"Loading ControlNet QR code model...\")\n",
    "        dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        \n",
    "        # ControlNet QR models (designed for SD v1.5)\n",
    "        controlnet_models = [\n",
    "            \"monster-labs/control_v1p_sd15_qrcode_monster\",\n",
    "            \"DionTimmer/controlnet_qrcode-control_v11p_sd15\",\n",
    "        ]\n",
    "        \n",
    "        controlnet = None\n",
    "        for model_id in controlnet_models:\n",
    "            try:\n",
    "                print(f\"Trying ControlNet model: {model_id}\")\n",
    "                controlnet = ControlNetModel.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=dtype,\n",
    "                    cache_dir=self.cache_dir\n",
    "                )\n",
    "                print(f\"Successfully loaded: {model_id}\")\n",
    "                self.controlnet_model = model_id\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if controlnet is None:\n",
    "            raise RuntimeError(\"Could not load any ControlNet QR code model\")\n",
    "        \n",
    "        # IMPORTANT: SD v1.5 ControlNet models are NOT compatible with SDXL\n",
    "        # Use SD v1.5 for ControlNet (proven compatibility)\n",
    "        is_sdxl = \"xl\" in self.model_id.lower() or \"sdxl\" in self.model_id.lower()\n",
    "        \n",
    "        if is_sdxl:\n",
    "            print(\"⚠️ Note: SD v1.5 ControlNet models are not compatible with SDXL architecture\")\n",
    "            print(\"Using SD v1.5 + ControlNet for QR generation (proven compatibility)\")\n",
    "            print(\"SDXL will be used for post-processing embedding method only\")\n",
    "            \n",
    "            # Use SD v1.5 for ControlNet (proven to work)\n",
    "            from diffusers import StableDiffusionControlNetPipeline\n",
    "            sd15_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "            self.controlnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "                sd15_model,\n",
    "                controlnet=controlnet,\n",
    "                torch_dtype=dtype,\n",
    "                cache_dir=self.cache_dir,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False\n",
    "            ).to(self.device)\n",
    "            print(\"✅ SD v1.5 ControlNet pipeline loaded (best compatibility for QR codes)\")\n",
    "        else:\n",
    "            # Use SD v1.5 directly\n",
    "            from diffusers import StableDiffusionControlNetPipeline\n",
    "            self.controlnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "                self.model_id,\n",
    "                controlnet=controlnet,\n",
    "                torch_dtype=dtype,\n",
    "                cache_dir=self.cache_dir,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False\n",
    "            ).to(self.device)\n",
    "            print(\"✅ SD v1.5 ControlNet pipeline loaded\")\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            try:\n",
    "                self.controlnet_pipe.enable_xformers_memory_efficient_attention()\n",
    "            except:\n",
    "                print(\"xformers not available, continuing without it\")\n",
    "        \n",
    "        print(\"ControlNet model ready!\")\n",
    "        return self.controlnet_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c431281d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:08.431484Z",
     "iopub.status.busy": "2026-02-25T07:03:08.430717Z",
     "iopub.status.idle": "2026-02-25T07:03:08.439875Z",
     "shell.execute_reply": "2026-02-25T07:03:08.439177Z",
     "shell.execute_reply.started": "2026-02-25T07:03:08.431458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Continue with embedding methods\n",
    "def embed_qr_artistically(base_image, qr_image, subtlety=0.88, contrast_boost=0.08):\n",
    "    \"\"\"Embed QR code into image artistically.\"\"\"\n",
    "    if base_image.mode != 'RGB':\n",
    "        base_image = base_image.convert('RGB')\n",
    "    if qr_image.mode != 'RGB':\n",
    "        qr_image = qr_image.convert('RGB')\n",
    "    \n",
    "    target_size = max(base_image.size)\n",
    "    base_image = base_image.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    qr_image = qr_image.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    base_array = np.array(base_image, dtype=float)\n",
    "    qr_array = np.array(qr_image, dtype=float)\n",
    "    \n",
    "    # Convert QR code to binary mask\n",
    "    qr_gray = np.dot(qr_array[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "    threshold = 127\n",
    "    qr_binary = (qr_gray < threshold).astype(float)\n",
    "    qr_mask_3d = np.expand_dims(qr_binary, axis=2)\n",
    "    \n",
    "    # Calculate contrast\n",
    "    min_contrast = 0.20\n",
    "    if subtlety <= 0.85:\n",
    "        effective_contrast = 0.25\n",
    "    elif subtlety <= 0.88:\n",
    "        effective_contrast = 0.25 - (subtlety - 0.85) * (0.25 - 0.22) / (0.88 - 0.85)\n",
    "    elif subtlety <= 0.90:\n",
    "        effective_contrast = 0.22 - (subtlety - 0.88) * (0.22 - 0.20) / (0.90 - 0.88)\n",
    "    else:\n",
    "        effective_contrast = min_contrast\n",
    "    effective_contrast = max(effective_contrast, min_contrast)\n",
    "    \n",
    "    dark_factor = 1.0 - effective_contrast\n",
    "    light_factor = 1.0 + effective_contrast\n",
    "    \n",
    "    result_array = base_array.copy()\n",
    "    \n",
    "    # Apply embedding\n",
    "    dark_mask = qr_mask_3d\n",
    "    result_array = result_array * (1 - dark_mask * (1 - dark_factor))\n",
    "    \n",
    "    light_mask = 1 - qr_mask_3d\n",
    "    result_array = result_array * (1 + light_mask * (light_factor - 1))\n",
    "    \n",
    "    # Contrast boost\n",
    "    if contrast_boost > 0:\n",
    "        qr_contrast = (qr_mask_3d - 0.5) * 2\n",
    "        contrast_multiplier = 100 if subtlety >= 0.88 else 80\n",
    "        contrast_strength = contrast_boost * contrast_multiplier\n",
    "        result_array = result_array + (qr_contrast * contrast_strength)\n",
    "    \n",
    "    # Pattern enhancement\n",
    "    qr_pattern_diff = (qr_mask_3d - 0.5) * 2\n",
    "    pattern_enhancement = 15.0\n",
    "    result_array = result_array + (qr_pattern_diff * pattern_enhancement)\n",
    "    \n",
    "    result_array = np.clip(result_array, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(result_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b9b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:08.619230Z",
     "iopub.status.busy": "2026-02-25T07:03:08.618437Z",
     "iopub.status.idle": "2026-02-25T07:03:08.634916Z",
     "shell.execute_reply": "2026-02-25T07:03:08.634165Z",
     "shell.execute_reply.started": "2026-02-25T07:03:08.619196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add methods to pipeline class\n",
    "def generate_with_controlnet(self, prompt, qr_data, output_path, image_size=512,\n",
    "                             num_inference_steps=30, guidance_scale=7.5,\n",
    "                             controlnet_conditioning_scale=1.5, seed=None,\n",
    "                             qr_enhancement_strength=0.20):\n",
    "    \"\"\"Generate artistic QR code using ControlNet.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ControlNet Artistic QR Code Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create QR code\n",
    "    print(\"\\n[Step 1/2] Creating QR code...\")\n",
    "    qr_image = self.create_qr_code(qr_data, size=image_size)\n",
    "    qr_image = qr_image.convert(\"RGB\")\n",
    "    \n",
    "    # Save original QR\n",
    "    qr_ref_path = output_path.replace('.png', '_original_qr.png')\n",
    "    qr_image.save(qr_ref_path)\n",
    "    print(f\"Original QR code saved: {qr_ref_path}\")\n",
    "    \n",
    "    # Load ControlNet\n",
    "    print(\"\\n[Step 2/2] Generating image with ControlNet...\")\n",
    "    pipe = self.load_controlnet_model()\n",
    "    \n",
    "    generator = None\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=self.device).manual_seed(seed)\n",
    "    \n",
    "    # Enhanced negative prompt for better artistic quality (matching qrcode-ai.com quality standards)\n",
    "    negative_prompt = \"blurry, distorted, unreadable qr, low quality, bad anatomy, bad proportions, deformed, ugly, disfigured, poorly drawn, bad hands, bad eyes, text, watermark, signature, grainy, noise, artifacts, oversaturated, underexposed, overexposed, jpeg artifacts, compression artifacts, low resolution, pixelated, broken QR code, unreadable QR pattern\"\n",
    "    \n",
    "    # Enhanced prompt optimized for AI Artistic QR Code generation (like qrcode-ai.com)\n",
    "    # Key: Balance artistic quality with QR scannability\n",
    "    # Keep prompt concise to avoid CLIP token truncation (77 token limit)\n",
    "    enhanced_prompt = f\"{prompt}, QR code pattern clearly visible, scannable QR code, high contrast modules, readable structure\"\n",
    "    \n",
    "    # Try generation with error handling for SDXL compatibility\n",
    "    try:\n",
    "        image = pipe(\n",
    "            prompt=enhanced_prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=qr_image,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "    except RuntimeError as e:\n",
    "        if \"mat1 and mat2 shapes cannot be multiplied\" in str(e) or \"dimension\" in str(e).lower():\n",
    "            print(f\"\\n⚠️ SDXL ControlNet incompatibility detected: {str(e)[:100]}...\")\n",
    "            print(\"Falling back to SD v1.5 + ControlNet (proven compatibility)...\")\n",
    "            \n",
    "            # Reload with SD v1.5\n",
    "            from diffusers import StableDiffusionControlNetPipeline\n",
    "            sd15_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "            dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "            \n",
    "            # Get the ControlNet model that was already loaded\n",
    "            controlnet = self.controlnet_pipe.controlnet if hasattr(self.controlnet_pipe, 'controlnet') else None\n",
    "            if controlnet is None:\n",
    "                # Reload ControlNet\n",
    "                from diffusers import ControlNetModel\n",
    "                controlnet = ControlNetModel.from_pretrained(\n",
    "                    self.controlnet_model,\n",
    "                    torch_dtype=dtype,\n",
    "                    cache_dir=self.cache_dir\n",
    "                )\n",
    "            \n",
    "            # Create SD v1.5 ControlNet pipeline\n",
    "            pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "                sd15_model,\n",
    "                controlnet=controlnet,\n",
    "                torch_dtype=dtype,\n",
    "                cache_dir=self.cache_dir,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False\n",
    "            ).to(self.device)\n",
    "            \n",
    "            if self.device == \"cuda\":\n",
    "                try:\n",
    "                    pipe.enable_xformers_memory_efficient_attention()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            print(\"✅ SD v1.5 ControlNet pipeline ready, retrying generation...\")\n",
    "            \n",
    "            # Retry with SD v1.5\n",
    "            image = pipe(\n",
    "                prompt=enhanced_prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=qr_image,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "            \n",
    "            # Update the pipeline for future use\n",
    "            self.controlnet_pipe = pipe\n",
    "        else:\n",
    "            raise  # Re-raise if it's a different error\n",
    "    \n",
    "    # Enhance QR scannability\n",
    "    print(\"\\n[Step 3/3] Enhancing QR code scannability...\")\n",
    "    image = enhance_qr_scannability(image, qr_image, qr_enhancement_strength)\n",
    "    \n",
    "    image.save(output_path)\n",
    "    print(f\"\\n✓ ControlNet artistic QR code saved to: {output_path}\")\n",
    "    return image\n",
    "\n",
    "def enhance_qr_scannability(generated_image, qr_reference, enhancement_strength=0.35):\n",
    "    \"\"\"\n",
    "    MAXIMUM scannability enhancement - ensures QR code is always visible and scannable.\n",
    "    Uses aggressive techniques to guarantee QR code readability.\n",
    "    \"\"\"\n",
    "    if generated_image.mode != 'RGB':\n",
    "        generated_image = generated_image.convert('RGB')\n",
    "    if qr_reference.mode != 'RGB':\n",
    "        qr_reference = qr_reference.convert('RGB')\n",
    "    \n",
    "    target_size = generated_image.size[0]\n",
    "    qr_reference = qr_reference.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    gen_array = np.array(generated_image, dtype=float)\n",
    "    qr_array = np.array(qr_reference, dtype=float)\n",
    "    \n",
    "    # Improved QR code mask extraction with Otsu thresholding\n",
    "    qr_gray = np.dot(qr_array[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "    \n",
    "    # Use Otsu's method for better threshold\n",
    "    try:\n",
    "        import cv2\n",
    "        qr_gray_uint8 = qr_gray.astype(np.uint8)\n",
    "        _, qr_binary_otsu = cv2.threshold(qr_gray_uint8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        threshold = np.mean(qr_binary_otsu)\n",
    "    except:\n",
    "        threshold = 127\n",
    "    \n",
    "    qr_binary = (qr_gray < threshold).astype(float)\n",
    "    qr_mask_3d = np.expand_dims(qr_binary, axis=2)\n",
    "    \n",
    "    result_array = gen_array.copy()\n",
    "    \n",
    "    # Technique 1: MAXIMUM darkening/lightening (0.85 = 85% contrast)\n",
    "    dark_mask = qr_mask_3d\n",
    "    darken_factor = 1.0 - (enhancement_strength * 0.85)  # Maximum: 0.85\n",
    "    result_array = result_array * (1 - dark_mask * (1 - darken_factor))\n",
    "    \n",
    "    light_mask = 1 - qr_mask_3d\n",
    "    lighten_factor = 1.0 + (enhancement_strength * 0.85)  # Maximum: 0.85\n",
    "    result_array = result_array * (1 + light_mask * (lighten_factor - 1))\n",
    "    \n",
    "    # Technique 2: MAXIMUM contrast boost (80x multiplier)\n",
    "    contrast_boost = enhancement_strength * 80\n",
    "    qr_contrast = (qr_mask_3d - 0.5) * 2\n",
    "    result_array = result_array + (qr_contrast * contrast_boost)\n",
    "    \n",
    "    # Technique 3: MAXIMUM edge enhancement (35x multiplier)\n",
    "    qr_pattern_diff = (qr_mask_3d - 0.5) * 2\n",
    "    edge_enhancement = enhancement_strength * 35\n",
    "    result_array = result_array + (qr_pattern_diff * edge_enhancement)\n",
    "    \n",
    "    # Technique 4: Local contrast enhancement\n",
    "    qr_enhancement_mask = np.abs(qr_pattern_diff)\n",
    "    local_contrast = enhancement_strength * 20\n",
    "    result_array = result_array + (qr_pattern_diff * local_contrast * qr_enhancement_mask)\n",
    "    \n",
    "    # Technique 5: Module contrast boost\n",
    "    module_contrast_boost = enhancement_strength * 15\n",
    "    result_array = result_array + (qr_pattern_diff * module_contrast_boost)\n",
    "    \n",
    "    # Technique 6: Binary enforcement - force near-binary values\n",
    "    qr_strength = np.abs(qr_pattern_diff)\n",
    "    binary_enforcement = enhancement_strength * 30\n",
    "    result_array = result_array + (qr_pattern_diff * binary_enforcement * qr_strength)\n",
    "    \n",
    "    # Clip values\n",
    "    result_array = np.clip(result_array, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Technique 7: Aggressive sharpening for maximum edge clarity\n",
    "    try:\n",
    "        from PIL import ImageFilter\n",
    "        result_image = Image.fromarray(result_array)\n",
    "        # Maximum sharpening: 200% with low threshold\n",
    "        result_image = result_image.filter(ImageFilter.UnsharpMask(radius=1, percent=200, threshold=2))\n",
    "        \n",
    "        # FALLBACK: If QR is still not visible enough, apply direct overlay\n",
    "        # Check if QR pattern is strong enough by comparing contrast\n",
    "        result_array_check = np.array(result_image, dtype=float)\n",
    "        qr_areas_dark = result_array_check * qr_mask_3d\n",
    "        qr_areas_light = result_array_check * (1 - qr_mask_3d)\n",
    "        avg_dark = np.mean(qr_areas_dark[qr_mask_3d[..., 0] > 0.5])\n",
    "        avg_light = np.mean(qr_areas_light[(1 - qr_mask_3d[..., 0]) > 0.5])\n",
    "        contrast_ratio = abs(avg_dark - avg_light) / 255.0\n",
    "        \n",
    "        # If contrast is too low (< 0.3), apply emergency overlay\n",
    "        if contrast_ratio < 0.3:\n",
    "            print(f\"⚠️ Low contrast detected ({contrast_ratio:.2f}), applying emergency QR overlay...\")\n",
    "            # Blend original QR directly with reduced opacity\n",
    "            qr_rgb = qr_array.copy()\n",
    "            # Convert QR to high contrast\n",
    "            qr_binary_rgb = np.stack([qr_binary, qr_binary, qr_binary], axis=2)\n",
    "            qr_black_white = qr_binary_rgb * 255.0  # Pure black/white\n",
    "            \n",
    "            # Overlay with 30% opacity for emergency visibility\n",
    "            overlay_strength = 0.30\n",
    "            result_array_final = result_array_check * (1 - overlay_strength) + qr_black_white * overlay_strength\n",
    "            result_array_final = np.clip(result_array_final, 0, 255).astype(np.uint8)\n",
    "            result_image = Image.fromarray(result_array_final)\n",
    "        \n",
    "        return result_image\n",
    "    except:\n",
    "        return Image.fromarray(result_array)\n",
    "\n",
    "# Attach methods to class\n",
    "ArtisticQRPipeline.generate_with_controlnet = generate_with_controlnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941271f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:52.954790Z",
     "iopub.status.busy": "2026-02-25T07:03:52.954245Z",
     "iopub.status.idle": "2026-02-25T07:03:52.962795Z",
     "shell.execute_reply": "2026-02-25T07:03:52.962057Z",
     "shell.execute_reply.started": "2026-02-25T07:03:52.954764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Complete pipeline method\n",
    "def process(self, prompt, qr_data, output_path, image_size=512, subtlety=0.88,\n",
    "           num_inference_steps=50, guidance_scale=7.5, seed=None,\n",
    "           use_controlnet=False, controlnet_conditioning_scale=1.5,\n",
    "           qr_enhancement_strength=0.20):\n",
    "    \"\"\"Complete pipeline: Generate image and embed QR code.\"\"\"\n",
    "    \n",
    "    if use_controlnet:\n",
    "        return self.generate_with_controlnet(\n",
    "            prompt=prompt, qr_data=qr_data, output_path=output_path,\n",
    "            image_size=image_size, num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale, controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            seed=seed, qr_enhancement_strength=qr_enhancement_strength\n",
    "        )\n",
    "    \n",
    "    # Post-processing embedding method\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Artistic QR Code Image Generation Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Create QR code\n",
    "    print(\"\\n[Step 1/3] Creating QR code...\")\n",
    "    qr_image = self.create_qr_code(qr_data, size=image_size)\n",
    "    \n",
    "    qr_ref_path = output_path.replace('.png', '_original_qr.png')\n",
    "    qr_image.save(qr_ref_path)\n",
    "    print(f\"Original QR code saved: {qr_ref_path}\")\n",
    "    \n",
    "    # Step 2: Generate artistic image\n",
    "    print(\"\\n[Step 2/3] Generating artistic image...\")\n",
    "    if self.pipe is None:\n",
    "        self.load_model()\n",
    "    \n",
    "    generator = None\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=self.device).manual_seed(seed)\n",
    "    \n",
    "    generated_image = self.pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    \n",
    "    generated_image = generated_image.resize((image_size, image_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Step 3: Embed QR code\n",
    "    print(\"\\n[Step 3/3] Embedding QR code artistically...\")\n",
    "    contrast_boost_value = 0.08 if subtlety >= 0.90 else 0.06\n",
    "    final_image = embed_qr_artistically(\n",
    "        generated_image, qr_image, subtlety=subtlety, contrast_boost=contrast_boost_value\n",
    "    )\n",
    "    \n",
    "    final_image.save(output_path)\n",
    "    print(f\"\\n✓ Artistic QR code image saved to: {output_path}\")\n",
    "    return final_image\n",
    "\n",
    "ArtisticQRPipeline.process = process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67371ea",
   "metadata": {},
   "source": [
    "## Generate Artistic QR Code\n",
    "\n",
    "Now let's generate an artistic QR code image. You can customize the parameters below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3f9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:55.616778Z",
     "iopub.status.busy": "2026-02-25T07:03:55.616424Z",
     "iopub.status.idle": "2026-02-25T07:03:55.624489Z",
     "shell.execute_reply": "2026-02-25T07:03:55.623678Z",
     "shell.execute_reply.started": "2026-02-25T07:03:55.616750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Configuration (Matching API Parameters)\n",
      "============================================================\n",
      "  Prompt: A dog looking up at the sky, white fluffy clouds in blue sky...\n",
      "  QR Data: podder_Soykot\n",
      "  Method: ControlNet\n",
      "  ControlNet Conditioning Scale: 2.0 (API range: 0.5-2.0, default: 1.5)\n",
      "  QR Enhancement Strength: 0.3 (API range: 0.0-0.3, default: 0.15)\n",
      "  Image Size: 512 (API range: 256-1024, default: 512)\n",
      "  Inference Steps: 50 (API range: 20-100, default: 50)\n",
      "  Guidance Scale: 7.5 (API range: 1.0-20.0, default: 7.5)\n",
      "  Seed: 42\n",
      "  Validate QR: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Optimized for AI Artistic QR Code generation (inspired by qrcode-ai.com)\n",
    "# Key principles: Artistic quality + High scannability\n",
    "PROMPT = \"A dog looking up at the sky, white fluffy clouds in blue sky, beautiful sunset colors, artistic illustration, detailed fur texture, expressive eyes, high quality, vibrant colors\"\n",
    "QR_DATA = \"podder_Soykot\"\n",
    "IMAGE_SIZE = 512  # API default: 512 (range: 256-1024)\n",
    "USE_CONTROLNET = True  # API default: False (post-processing), True for ControlNet\n",
    "SEED = 42  # API: Optional[int], None for random\n",
    "\n",
    "# ControlNet parameters (only used if USE_CONTROLNET=True)\n",
    "# API defaults: controlnet_conditioning_scale=1.5 (range: 0.5-2.0), qr_enhancement_strength=0.15 (range: 0.0-0.3)\n",
    "# Using maximum scannability settings (can be adjusted based on needs)\n",
    "CONTROLNET_CONDITIONING_SCALE = 2.0  # API max: 2.0 (default: 1.5) - MAXIMUM for best scannability\n",
    "QR_ENHANCEMENT_STRENGTH = 0.30  # API max: 0.3 (default: 0.15) - Increased for better scannability\n",
    "\n",
    "# Post-processing parameters (only used if USE_CONTROLNET=False)\n",
    "# API default: subtlety=0.92 (range: 0.85-0.95)\n",
    "SUBTLETY = 0.92  # API default\n",
    "\n",
    "# Generation parameters - matches API defaults\n",
    "NUM_INFERENCE_STEPS = 50  # API default: 50 (range: 20-100)\n",
    "GUIDANCE_SCALE = 7.5  # API default: 7.5 (range: 1.0-20.0)\n",
    "\n",
    "# Validation\n",
    "VALIDATE_QR = True  # API default: True\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Configuration (Matching API Parameters)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Prompt: {PROMPT[:60]}...\")\n",
    "print(f\"  QR Data: {QR_DATA}\")\n",
    "print(f\"  Method: {'ControlNet' if USE_CONTROLNET else 'Post-processing Embedding'}\")\n",
    "if USE_CONTROLNET:\n",
    "    print(f\"  ControlNet Conditioning Scale: {CONTROLNET_CONDITIONING_SCALE} (API range: 0.5-2.0, default: 1.5)\")\n",
    "    print(f\"  QR Enhancement Strength: {QR_ENHANCEMENT_STRENGTH} (API range: 0.0-0.3, default: 0.15)\")\n",
    "else:\n",
    "    print(f\"  Subtlety: {SUBTLETY} (API range: 0.85-0.95, default: 0.92)\")\n",
    "print(f\"  Image Size: {IMAGE_SIZE} (API range: 256-1024, default: 512)\")\n",
    "print(f\"  Inference Steps: {NUM_INFERENCE_STEPS} (API range: 20-100, default: 50)\")\n",
    "print(f\"  Guidance Scale: {GUIDANCE_SCALE} (API range: 1.0-20.0, default: 7.5)\")\n",
    "print(f\"  Seed: {SEED if SEED is not None else 'Random'}\")\n",
    "print(f\"  Validate QR: {VALIDATE_QR}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5efff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T07:03:57.754948Z",
     "iopub.status.busy": "2026-02-25T07:03:57.754665Z",
     "iopub.status.idle": "2026-02-25T07:05:43.308357Z",
     "shell.execute_reply": "2026-02-25T07:05:43.307142Z",
     "shell.execute_reply.started": "2026-02-25T07:03:57.754926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting QR Code Generation\n",
      "============================================================\n",
      "Method: ControlNet\n",
      "QR Data: podder_Soykot\n",
      "Prompt: A dog looking up at the sky, white fluffy clouds in blue sky, beautiful sunset c...\n",
      "============================================================\n",
      "============================================================\n",
      "ControlNet Artistic QR Code Generation\n",
      "============================================================\n",
      "\n",
      "[Step 1/2] Creating QR code...\n",
      "Original QR code saved: /kaggle/working/outputs/artistic_qr_output_original_qr.png\n",
      "\n",
      "[Step 2/2] Generating image with ControlNet...\n",
      "Loading ControlNet QR code model for SDXL...\n",
      "⭐ SDXL + ControlNet: Best for artistic visuals where QR should still scan\n",
      "Trying ControlNet model: monster-labs/control_v1p_sd15_qrcode_monster\n",
      "Successfully loaded: monster-labs/control_v1p_sd15_qrcode_monster\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efa5bc02d834c9889a8e37be7ae99f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ef8bae889746b1988d5359db807e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633a85c4e71c4ad0ad3447e097b03dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8f4b014f804a90bf54a3bc00d6f294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8b87687e9d4c7ab2943df4752995e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None, 'requires_safety_checker': False} are not expected by StableDiffusionXLControlNetPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a982292c5025421ab31dd42a034f14bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ea58fac73743d5aa55a0e299173774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14cfad433db4ef08b7d181dabb0b63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['detailed, professional']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['detailed, professional']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xformers not available, continuing without it\n",
      "⭐ SDXL ControlNet model loaded successfully!\n",
      "   Best for: Artistic visuals where QR should still scan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4180749b71b44a2bcb9a3b3bd481afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/25124061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Call process method exactly as API does (see app.py line 349-361)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m final_image = pipeline.process(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROMPT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mqr_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQR_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/2403242302.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, prompt, qr_data, output_path, image_size, subtlety, num_inference_steps, guidance_scale, seed, use_controlnet, controlnet_conditioning_scale, qr_enhancement_strength)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_controlnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         return self.generate_with_controlnet(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqr_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/4044917118.py\u001b[0m in \u001b[0;36mgenerate_with_controlnet\u001b[0;34m(self, prompt, qr_data, output_path, image_size, num_inference_steps, guidance_scale, controlnet_conditioning_scale, seed, qr_enhancement_strength)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0menhanced_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{prompt}, QR code pattern clearly visible, scannable QR code, high contrast black and white modules, readable QR code structure, high quality, detailed, professional\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     image = pipe(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menhanced_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, prompt_2, image, height, width, num_inference_steps, timesteps, sigmas, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                     \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrolnet_cond_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcontrolnet_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                 down_block_res_samples, mid_block_res_sample = self.controlnet(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                     \u001b[0mcontrol_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/controlnets/controlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, controlnet_cond, conditioning_scale, class_labels, timestep_cond, attention_mask, added_cond_kwargs, cross_attention_kwargs, guess_mode, return_dict)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdownsample_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownsample_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdownsample_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                 sample, res_samples = downsample_block(\n\u001b[0m\u001b[1;32m    744\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m                 hidden_states = attn(\n\u001b[0m\u001b[1;32m   1271\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/transformers/transformer_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 )\n\u001b[1;32m    426\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 hidden_states = block(\n\u001b[0m\u001b[1;32m    428\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mnorm_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             attn_output = self.attn2(\n\u001b[0m\u001b[1;32m   1044\u001b[0m                 \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mcross_attention_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattn_parameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         return self.processor(\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_encoder_hidden_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline with SDXL for best artistic quality\n",
    "# ⭐ SDXL provides: Higher artistic quality, richer details, better composition\n",
    "pipeline = ArtisticQRPipeline(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",  # SDXL for better quality\n",
    "    cache_dir=HF_CACHE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Generate QR code (matching API process method call)\n",
    "output_filename = \"artistic_qr_output.png\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting QR Code Generation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: {'ControlNet' if USE_CONTROLNET else 'Post-processing Embedding'}\")\n",
    "print(f\"QR Data: {QR_DATA}\")\n",
    "print(f\"Prompt: {PROMPT[:80]}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Call process method exactly as API does (see app.py line 349-361)\n",
    "final_image = pipeline.process(\n",
    "    prompt=PROMPT,\n",
    "    qr_data=QR_DATA,\n",
    "    output_path=output_path,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    subtlety=SUBTLETY,\n",
    "    num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    seed=SEED,\n",
    "    use_controlnet=USE_CONTROLNET,\n",
    "    controlnet_conditioning_scale=CONTROLNET_CONDITIONING_SCALE,\n",
    "    qr_enhancement_strength=QR_ENHANCEMENT_STRENGTH\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ Generation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Output saved to: {output_path}\")\n",
    "\n",
    "# Also save original QR reference if it exists\n",
    "qr_ref_path = output_path.replace('.png', '_original_qr.png')\n",
    "if os.path.exists(qr_ref_path):\n",
    "    print(f\"Original QR saved to: {qr_ref_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531d813",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-25T07:05:43.309862Z",
     "iopub.status.idle": "2026-02-25T07:05:43.310145Z",
     "shell.execute_reply": "2026-02-25T07:05:43.310034Z",
     "shell.execute_reply.started": "2026-02-25T07:05:43.310021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display the generated image\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "display(IPImage(output_path, width=512))\n",
    "print(f\"\\nGenerated QR Code Image\")\n",
    "print(f\"QR Data: {QR_DATA}\")\n",
    "\n",
    "# Also display original QR for comparison\n",
    "qr_ref_path = output_path.replace('.png', '_original_qr.png')\n",
    "if os.path.exists(qr_ref_path):\n",
    "    print(\"\\nOriginal QR Code (for comparison):\")\n",
    "    display(IPImage(qr_ref_path, width=256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ae7f2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-25T07:05:43.311431Z",
     "iopub.status.idle": "2026-02-25T07:05:43.311729Z",
     "shell.execute_reply": "2026-02-25T07:05:43.311573Z",
     "shell.execute_reply.started": "2026-02-25T07:05:43.311560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validate QR code scannability (matching API validation - see app.py line 372-383)\n",
    "# API uses QRCodeValidator.validate_qr_code() and assess_scannability_level()\n",
    "if VALIDATE_QR:\n",
    "    try:\n",
    "        import cv2\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"QR Code Validation (Matching API)\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def preprocess_image_for_qr(img):\n",
    "        \"\"\"Preprocess image to improve QR code detection.\"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply adaptive thresholding to enhance contrast\n",
    "        adaptive_thresh = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        \n",
    "        # Also try Otsu's thresholding\n",
    "        _, otsu_thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Increase contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        contrast_enhanced = clahe.apply(gray)\n",
    "        \n",
    "        return [img, gray, adaptive_thresh, otsu_thresh, contrast_enhanced]\n",
    "    \n",
    "    def validate_qr(image_path, expected_data=None):\n",
    "        \"\"\"Validate QR code scannability using OpenCV with preprocessing.\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return {\"scannable\": False, \"error\": \"Could not read image\"}\n",
    "        \n",
    "        detector = cv2.QRCodeDetector()\n",
    "        \n",
    "        # Try different preprocessing methods\n",
    "        processed_images = preprocess_image_for_qr(img)\n",
    "        methods = [\"Original\", \"Grayscale\", \"Adaptive Threshold\", \"Otsu Threshold\", \"Contrast Enhanced\"]\n",
    "        \n",
    "        for processed_img, method_name in zip(processed_images, methods):\n",
    "            # Try detectAndDecodeMulti first (returns 4 values)\n",
    "            try:\n",
    "                retval, decoded_info, points, straight_qrcode = detector.detectAndDecodeMulti(processed_img)\n",
    "                if retval and decoded_info is not None:\n",
    "                    # decoded_info is a tuple/list of strings\n",
    "                    for info in decoded_info:\n",
    "                        if info is not None:\n",
    "                            # Handle both string and numpy array cases\n",
    "                            if isinstance(info, str):\n",
    "                                decoded_str = info.strip()\n",
    "                            elif isinstance(info, (np.ndarray, list, tuple)):\n",
    "                                # Check if it's coordinates (numbers) - skip those\n",
    "                                if len(info) > 0:\n",
    "                                    first_elem = info[0] if hasattr(info, '__getitem__') else info\n",
    "                                    if isinstance(first_elem, (int, float, np.number)):\n",
    "                                        # This is likely points, not decoded data - skip\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        decoded_str = str(info).strip()\n",
    "                                else:\n",
    "                                    continue\n",
    "                            else:\n",
    "                                decoded_str = str(info).strip()\n",
    "                            \n",
    "                            # Only accept if it's a reasonable string (not coordinates)\n",
    "                            # Check if it looks like coordinates (contains brackets and numbers)\n",
    "                            is_coordinates = ('[' in decoded_str and ']' in decoded_str) or \\\n",
    "                                           (decoded_str.replace('.', '').replace('-', '').replace(' ', '').replace('[', '').replace(']', '').isdigit())\n",
    "                            \n",
    "                            if len(decoded_str) > 0 and not is_coordinates:\n",
    "                                matches = (decoded_str == expected_data) if expected_data else True\n",
    "                                return {\n",
    "                                    \"scannable\": True,\n",
    "                                    \"data_decoded\": decoded_str,\n",
    "                                    \"matches_expected\": matches,\n",
    "                                    \"method\": f\"OpenCV (Multi, {method_name})\"\n",
    "                                }\n",
    "            except (ValueError, cv2.error, TypeError):\n",
    "                pass\n",
    "            \n",
    "            # Try single decode (returns 3 values: retval, decoded_info, points)\n",
    "            try:\n",
    "                retval, decoded_info, points = detector.detectAndDecode(processed_img)\n",
    "                # Check if retval is True and decoded_info is not empty\n",
    "                if retval and decoded_info is not None:\n",
    "                    # Handle both string and numpy array cases\n",
    "                    if isinstance(decoded_info, str):\n",
    "                        decoded_str = decoded_info.strip()\n",
    "                    elif isinstance(decoded_info, (np.ndarray, list, tuple)):\n",
    "                        # If it's an array, it might be points - skip it\n",
    "                        # Try to extract string if it's a structured array\n",
    "                        if len(decoded_info) > 0:\n",
    "                            # Check if first element looks like coordinates (numbers)\n",
    "                            first_elem = decoded_info[0] if hasattr(decoded_info, '__getitem__') else decoded_info\n",
    "                            if isinstance(first_elem, (int, float, np.number)):\n",
    "                                # This is likely points, not decoded data - skip\n",
    "                                continue\n",
    "                            else:\n",
    "                                decoded_str = str(decoded_info).strip()\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        decoded_str = str(decoded_info).strip()\n",
    "                    \n",
    "                    # Only accept if it's a reasonable string (not coordinates)\n",
    "                    # Check if it looks like coordinates (contains brackets and numbers)\n",
    "                    is_coordinates = ('[' in decoded_str and ']' in decoded_str) or \\\n",
    "                                   (decoded_str.replace('.', '').replace('-', '').replace(' ', '').replace('[', '').replace(']', '').isdigit())\n",
    "                    \n",
    "                    if len(decoded_str) > 0 and not is_coordinates:\n",
    "                        matches = (decoded_str == expected_data) if expected_data else True\n",
    "                        return {\n",
    "                            \"scannable\": True,\n",
    "                            \"data_decoded\": decoded_str,\n",
    "                            \"matches_expected\": matches,\n",
    "                            \"method\": f\"OpenCV ({method_name})\"\n",
    "                        }\n",
    "            except (cv2.error, ValueError, TypeError) as e:\n",
    "                continue\n",
    "        \n",
    "        return {\"scannable\": False, \"error\": \"Could not decode QR code after preprocessing\", \"method\": \"OpenCV\"}\n",
    "    \n",
    "    # Validate using OpenCV with preprocessing (matching API validation)\n",
    "    print(f\"Validating QR code: {output_filename}\")\n",
    "    print(f\"Expected data: {QR_DATA}\")\n",
    "    result = validate_qr(output_path, QR_DATA)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QR Code Validation Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Scannable: {'✅ YES' if result['scannable'] else '❌ NO'}\")\n",
    "    if result.get('data_decoded'):\n",
    "        print(f\"Decoded Data: {result['data_decoded']}\")\n",
    "        if 'matches_expected' in result:\n",
    "            print(f\"Matches Expected: {'✅ YES' if result['matches_expected'] else '❌ NO'}\")\n",
    "    if result.get('method'):\n",
    "        print(f\"Method Used: {result['method']}\")\n",
    "    if result.get('error'):\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Provide recommendations\n",
    "    if not result['scannable']:\n",
    "        print(\"\\n💡 Recommendations to improve scannability:\")\n",
    "        print(\"   Current settings: CONDITIONING_SCALE={}, ENHANCEMENT_STRENGTH={}\".format(\n",
    "            CONTROLNET_CONDITIONING_SCALE, QR_ENHANCEMENT_STRENGTH))\n",
    "        if CONTROLNET_CONDITIONING_SCALE < 2.0:\n",
    "            print(\"   1. Try maximum CONTROLNET_CONDITIONING_SCALE = 2.0 (API max)\")\n",
    "        if QR_ENHANCEMENT_STRENGTH < 0.30:\n",
    "            print(\"   2. Try maximum QR_ENHANCEMENT_STRENGTH = 0.30 (API max: 0.3)\")\n",
    "        print(\"   3. ⚠️  IMPORTANT: Test with a phone camera!\")\n",
    "        print(\"      Phone scanners are often more tolerant than OpenCV validation\")\n",
    "        print(\"      Many artistic QR codes work on phones even if validation fails\")\n",
    "        print(\"   4. Try post-processing method with subtlety = 0.85\")\n",
    "        print(\"   5. Consider using a simpler prompt (less artistic elements)\")\n",
    "    elif result.get('data_decoded') and not result.get('matches_expected', True):\n",
    "        # QR detected but data doesn't match (might be coordinates)\n",
    "        print(\"\\n⚠️  QR code structure detected but data could not be decoded.\")\n",
    "        print(\"   This means the QR pattern is visible but too subtle for OpenCV to read.\")\n",
    "        print(\"   💡 Try testing with a phone camera - it often works better!\")\n",
    "    \n",
    "        print(\"\\nNote: PyZBar requires system libraries not available on Kaggle.\")\n",
    "        print(\"Using OpenCV for validation, which is more reliable in this environment.\")\n",
    "        print(\"API uses QRCodeValidator class for validation (see qr_validator.py)\")\n",
    "    \n",
    "    except ImportError as e:\n",
    "        print(f\"⚠️  QR validation libraries not available: {e}\")\n",
    "        print(\"   Install opencv-python-headless for validation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Validation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n⚠️ Validation skipped (VALIDATE_QR=False)\")\n",
    "    print(\"   Set VALIDATE_QR=True to enable validation (API default: True)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c34ed3",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **⭐ AI Artistic QR Code Generation** (inspired by [qrcode-ai.com](https://qrcode-ai.com/))\n",
    "  - Uses **ControlNet + Stable Diffusion XL** for seamless QR integration\n",
    "  - Optimized for **High Scannability** while maintaining artistic quality\n",
    "  - Auto-selects QR version for optimal size and error correction\n",
    "  \n",
    "- **Output files** are saved to `/kaggle/working/outputs/`\n",
    "- **Models** are cached in `/kaggle/working/hf_cache/`\n",
    "- **GPU** is automatically detected and used if available\n",
    "- **This notebook matches the API implementation** (see `app.py` for reference)\n",
    "- **ControlNet** method generally produces more scannable QR codes (industry standard)\n",
    "\n",
    "## How This Compares to qrcode-ai.com\n",
    "\n",
    "**Similarities:**\n",
    "- ✅ AI-powered artistic QR code generation\n",
    "- ✅ ControlNet-based approach (industry standard)\n",
    "- ✅ High scannability focus\n",
    "- ✅ Artistic quality preservation\n",
    "- ✅ Multiple format support (PNG)\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Auto QR version selection for optimal size\n",
    "- ✅ Pure black/white QR modules for maximum contrast\n",
    "- ✅ Enhanced scannability post-processing\n",
    "- ✅ Scannability validation with OpenCV\n",
    "\n",
    "## API Parameter Ranges (Matching app.py)\n",
    "\n",
    "### Post-Processing Method (`use_controlnet=False`)\n",
    "- `subtlety`: 0.85-0.95 (API default: 0.92) - lower = more visible QR\n",
    "\n",
    "### ControlNet Method (`use_controlnet=True`)\n",
    "- `controlnet_conditioning_scale`: 0.5-2.0 (API default: 1.5) - higher = stronger QR structure\n",
    "- `qr_enhancement_strength`: 0.0-0.3 (API default: 0.15) - higher = more visible QR\n",
    "\n",
    "### Common Parameters\n",
    "- `image_size`: 256-1024 (API default: 512)\n",
    "- `num_inference_steps`: 20-100 (API default: 50)\n",
    "- `guidance_scale`: 1.0-20.0 (API default: 7.5)\n",
    "- `seed`: Optional[int] (API default: None for random)\n",
    "- `validate_qr`: bool (API default: True)\n",
    "\n",
    "## Tips for Better Scannability\n",
    "\n",
    "1. **Use ControlNet** (`use_controlnet=True`) for better results\n",
    "2. **Maximum settings** for best scannability:\n",
    "   - `controlnet_conditioning_scale = 2.0` (API max)\n",
    "   - `qr_enhancement_strength = 0.30` (API max: 0.3)\n",
    "3. **For post-processing**: Lower `subtlety` (0.85-0.88) for more visible QR\n",
    "4. **Test with phone camera** - sometimes works even if validation fails\n",
    "\n",
    "## API Usage\n",
    "\n",
    "To use the API instead of this notebook:\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/api/v1/generate\", json={\n",
    "    \"prompt\": \"A dog in the sky with clouds\",\n",
    "    \"qr_data\": \"https://example.com\",\n",
    "    \"use_controlnet\": True,\n",
    "    \"controlnet_conditioning_scale\": 2.0,\n",
    "    \"qr_enhancement_strength\": 0.30,\n",
    "    \"validate_qr\": True\n",
    "})\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
